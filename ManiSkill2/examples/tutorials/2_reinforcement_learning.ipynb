{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/haosulab/ManiSkill2/blob/main/examples/tutorials/2_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T7S6wmryEub"
   },
   "source": [
    "# Setup Code\n",
    "\n",
    "To begin, prepare the colab environment by clicking the play button below and make sure you are using a GPU runtime. This will install all dependencies for the future code. This can take up to 1.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kYugcL-gfwfz",
    "outputId": "3b2cf256-95fc-4e23-d1b8-f4502b6b3467",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# below fixes some bugs introduced by some recent Colab changes\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill2/main/docker/nvidia_icd.json\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill2/main/docker/10_nvidia.json\n",
    "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
    "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
    "# dependencies\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev\n",
    "!pip install stable_baselines3 mani_skill2\n",
    "!pip install --upgrade --no-cache-dir gdown"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nWP-k5bbvC5C"
   },
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import site\n",
    "    site.main() # run this so local pip installs are recognized"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_jwRVKoPRXs"
   },
   "source": [
    "# Robotic Learning Tutorial Part 1: Reinforcement Learning\n",
    "\n",
    "This notebook will go over a Reinforcement Learning (RL) baseline solving the [ManiSkill](https://github.com/haosulab/ManiSkill2) environments. We will be using the [Stable Baselines 3 (SB3)](https://github.com/DLR-RM/stable-baselines3) package and the LiftCube enviornment as part of this tutorial with state and RGBD observations. Specifically,  we will use [PPO](https://openai.com/blog/openai-baselines-ppo/) algorithm to train agents.\n",
    "\n",
    "A single-file code version of both the state based and visual based RL tutorial can be found here: https://github.com/haosulab/ManiSkill2/tree/main/examples/tutorials/reinforcement-learning\n",
    "\n",
    "Section 1 covers State Based RL and Section 2 covers Visual RL. They are both self-contained so you can skip either section and run the other without any errors, but you must run all code before Section 1. Importantly, Section 2 covers the ManiSkill VecEnv, which brings a massive speedup to visual RL relative to other libraries by returning observations on the GPU (supporting PyTorch and Jax tensors on the GPU).\n",
    "\n",
    "First, we will import some packages shared by both sections and define some useful wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "shSyf4FQPR22"
   },
   "source": [
    "# Import required packages\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as spaces\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import mani_skill2.envs\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch as th"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJuDRHixXPw3"
   },
   "source": [
    "## 0 Useful Wrappers\n",
    "\n",
    "For state and visual observations, there are some useful wrappers we recommend using to improve RL training and monitoring. Namely, we recommend treating tasks as continuous tasks with infinite horizon. This means `done` is always `False` until a time limit is hit.\n",
    "\n",
    "Libraries like SB3 handle this by checking for a `info[TimeLimit.truncated]` value. The `ContinuousTaskWrapper` does this and also lets you specify your own time limit with `max_episode_steps`.\n",
    "\n",
    "To record success rates during evaluation we also provide a `SuccessInfoWrapper` here which allows SB3 to record success rates while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dP4cuYHWX0GL"
   },
   "source": [
    "# the following wrappers are importable from mani_skill2.utils.wrappers.sb3\n",
    "# Defines a continuous, infinite horizon, task where done is always False\n",
    "# unless a timelimit is reached.\n",
    "class ContinuousTaskWrapper(gym.Wrapper):\n",
    "    def __init__(self, env) -> None:\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return super().reset(*args, **kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, rew, terminated, truncated, info = super().step(action)\n",
    "        return ob, rew, False, truncated, info\n",
    "\n",
    "# A simple wrapper that adds a is_success key which SB3 tracks\n",
    "class SuccessInfoWrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        ob, rew, terminated, truncated, info = super().step(action)\n",
    "        info[\"is_success\"] = info[\"success\"]\n",
    "        return ob, rew, terminated, truncated, info"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QmzX6177XYr"
   },
   "source": [
    "## 1 State Based RL\n",
    "\n",
    "In state based RL, the observations are all flat/dense vectors and easy to learn from for machine learning algorithms.\n",
    "\n",
    "State based solving is generally much faster as generating visual observations is slow, especially without a GPU. Moreover, distillation of information from images is difficult, and state may provide privileged information about the environment that makes solving easier. However state based policies are more limited in their generalizability across tasks and objects without additional techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYkXMopo7bU-"
   },
   "source": [
    "### 1.1 Setting up Training and Evaluation Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK3uvUwuoOJx"
   },
   "source": [
    "We first need to setup training and evaluation environments for SB3 to use in addition to adding the custom wrappers we defined in section 0.\n",
    "\n",
    "Note that ManiSkill2 uses GPU-optimized vectorized environments to massively speed up the simulation of environments with rendering (e.g., in visual RL). If rendering is not needed (e.g., in this case, state based RL), you do not need to use the GPU-optimized vectorized environment of ManiSkill2. For simplicity, in this state-based RL example, we just use SB3's vectorized environment to parallelize environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bTCao9c29Qd0"
   },
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from mani_skill2.utils.wrappers import RecordEpisode\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "num_envs = 2 # you can increases this and decrease the n_steps parameter if you have more cores to speed up training\n",
    "env_id = \"LiftCube-v0\"\n",
    "obs_mode = \"state\"\n",
    "control_mode = \"pd_ee_delta_pose\"\n",
    "reward_mode = \"normalized_dense\" # this the default reward mode which is a dense reward scaled to [0, 1]\n",
    "\n",
    "# define an SB3 style make_env function for evaluation\n",
    "def make_env(env_id: str, max_episode_steps: int = None, record_dir: str = None):\n",
    "    def _init() -> gym.Env:\n",
    "        # NOTE: Import envs here so that they are registered with gym in subprocesses\n",
    "        import mani_skill2.envs\n",
    "        env = gym.make(env_id, obs_mode=obs_mode, reward_mode=reward_mode, control_mode=control_mode, max_episode_steps=max_episode_steps, render_mode=\"cameras\")\n",
    "        # For training, we regard the task as a continuous task with infinite horizon.\n",
    "        # you can use the ContinuousTaskWrapper here for that\n",
    "        if max_episode_steps is not None:\n",
    "            env = ContinuousTaskWrapper(env)\n",
    "        if record_dir is not None:\n",
    "            env = SuccessInfoWrapper(env)\n",
    "            env = RecordEpisode(\n",
    "                env, record_dir, info_on_video=True\n",
    "            )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# create one eval environment\n",
    "eval_env = SubprocVecEnv([make_env(env_id, record_dir=\"logs/videos\") for i in range(1)])\n",
    "eval_env = VecMonitor(eval_env) # attach this so SB3 can log reward metrics\n",
    "eval_env.seed(0)\n",
    "eval_env.reset()\n",
    "\n",
    "# create num_envs training environments\n",
    "# we also specify max_episode_steps=50 to speed up training\n",
    "env = SubprocVecEnv([make_env(env_id, max_episode_steps=50) for i in range(num_envs)])\n",
    "env = VecMonitor(env)\n",
    "env.seed(0)\n",
    "obs = env.reset()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imh9MIL0-xBg"
   },
   "source": [
    "To help monitor our training, with SB3, we can create an evaluation callback function as well as a checkpoint callback function. The `eval_env` will also save videos to `logs/videos`. Whenever the evaluation reward has improved, it will save a new best model as well. Finally, the checkpoint callback will periodically save the training progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fgAZKTBc-xBh"
   },
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# SB3 uses callback functions to create evaluation and checkpoints\n",
    "\n",
    "# Evaluation: periodically evaluate the agent without noise and save results to the logs folder\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
    "                         log_path=\"./logs/\", eval_freq=32000,\n",
    "                         deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=32000,\n",
    "    save_path=\"./logs/\",\n",
    "    name_prefix=\"rl_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRxovjJc7gP-"
   },
   "source": [
    "### 1.2 RL Training with PPO\n",
    "\n",
    "Finally, we can begin training. We have to first define the policy and training configuration. The configs provided are already tuned  and will be able to train out a successful LiftCube policy. The configs are set to perform a rollout of 3200 steps split across each parallel environment, and update the policy with batch size 400 for 5 epochs. All logs, including tensorboard logs (run `tensorboard --logdir logs` to view them) and evaluation videos, are stored in the `logs/videos` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCX8dfBn-r_2",
    "outputId": "413f91b8-2617-4cee-9768-dff22aaf30d7"
   },
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "set_random_seed(0) # set SB3's global seed to 0\n",
    "rollout_steps = 3200\n",
    "\n",
    "# create our model\n",
    "policy_kwargs = dict(net_arch=[256, 256])\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,\n",
    "    n_steps=rollout_steps // num_envs, batch_size=400,\n",
    "    n_epochs=15,\n",
    "    tensorboard_log=\"./logs\",\n",
    "    gamma=0.85,\n",
    "    target_kl=0.05\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ocm_YIVy_CbV"
   },
   "source": [
    "Run the next cell to train the model for 300,000+ steps. The final model is then saved to `logs/latest_model`. This can take 5 minutes to 50 minutes (depending on your machine) to finish training. To speed up training you can use a computer with more CPU cores and/or a more powerful GPU.\n",
    "\n",
    "To keep track of training progress you can go to `logs/videos` and download the evaluation videos saved during training.\n",
    "\n",
    "We have also provided weights trained through this colab tutorial so if you wish to skip the training time you can skip the next cell and run the following one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZByxcv2_CbW"
   },
   "source": [
    "# Train with PPO\n",
    "model.learn(300_000, callback=[checkpoint_callback, eval_callback])\n",
    "model.save(\"./logs/latest_model\")\n",
    "\n",
    "# optionally load back the model that was saved\n",
    "model = model.load(\"./logs/latest_model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l8yYqhGY_CbX"
   },
   "source": [
    "# Code for simply loading a pretrained policy\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://huggingface.co/datasets/haosulab/ManiSkill2/resolve/main/pretrained_models/tutorials/LiftCube-v0_rl.state.pd_ee_delta_pose.zip\",\n",
    "    \"pretrained_model.zip\")\n",
    "model.policy = model.load(\"pretrained_model\").policy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx0lALVT_Soi"
   },
   "source": [
    "### 1.3 Evaluation\n",
    "Once a model is trained, whether you ran the script above or downloaded the pretrained policy, you can run below to evaluate it and save some videos. You can set `render=True` if you have a GUI to create to live render the evaluation.\n",
    "\n",
    "If you use default configurations / use the pretrained model, you should get 80%+ success rate on LiftCube. If you train longer you can get near 100%.\n",
    "\n",
    "**Note that the green sphere in the videos represents the target height to lift the cube to (not a goal position).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9nPNZYKG_Soj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "856f71d4-b7f1-44c5-de37-6eee38f97290"
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "eval_env.close() # close the old eval env\n",
    "# make a new one that saves to a different directory\n",
    "eval_env = SubprocVecEnv([make_env(env_id, record_dir=\"logs/eval_videos\") for i in range(1)])\n",
    "eval_env = VecMonitor(eval_env) # attach this so SB3 can log reward metrics\n",
    "eval_env.seed(1)\n",
    "eval_env.reset()\n",
    "\n",
    "returns, ep_lens = evaluate_policy(model, eval_env, deterministic=True, render=False, return_episode_rewards=True, n_eval_episodes=10)\n",
    "success = np.array(ep_lens) < 200 # episode length < 200 means we solved the task before time ran out\n",
    "success_rate = success.mean()\n",
    "print(f\"Success Rate: {success_rate}\")\n",
    "print(f\"Episode Lengths: {ep_lens}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "-LkndEPa_Sok",
    "outputId": "f0a203ef-0f56-4221-846c-23d28c8c8b18"
   },
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./logs/eval_videos/2.mp4\", embed=True) # Watch one of the replays"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFT6tkeR7mfU"
   },
   "source": [
    "## 2 Visual RL\n",
    "\n",
    "In Visual RL, the agent's observations are now visual, whether its an RGBD image or a point cloud. Vision based policies learn to extract task-oriented representation, but are more difficult to train and need more work. Section 2 here will cover the necessary steps to get a basic visual RL agent working on the LiftCube environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LofwfGDlPavE"
   },
   "source": [
    "First, we note that SB3 won't work out of the box due to observations including depth data as well as being in a format different to what SB3 requires. To remind ourselves, lets create an environment and inspect the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "NNT4Fi6ZPTa4",
    "outputId": "6c60fc7f-2ee4-4e4a-e401-c306c3a0aea2"
   },
   "source": [
    "env_id = \"LiftCube-v0\"\n",
    "obs_mode = \"rgbd\"\n",
    "control_mode = \"pd_ee_delta_pose\"\n",
    "reward_mode = \"normalized_dense\" # this the default reward mode which is a dense reward scaled to [0, 1]\n",
    "# create our environment with our configs and then reset to a clean state\n",
    "env = gym.make(env_id, obs_mode=obs_mode, reward_mode=reward_mode, control_mode=control_mode, render_mode=\"cameras\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# take a look at the current state\n",
    "img = env.unwrapped.render_cameras()\n",
    "plt.imshow(img)\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ih_PagxUPe8Q",
    "outputId": "58a0c50b-c7bd-4950-898a-1f189d6701d5"
   },
   "source": [
    "# the observations\n",
    "print(\"The raw observation\", obs.keys())\n",
    "print(\"The data in the observation:\")\n",
    "print(\"image\", obs[\"image\"].keys())\n",
    "print(\"agent\", obs[\"agent\"].keys())\n",
    "print(\"extra\", obs[\"extra\"].keys())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF-xYcxqmkC9"
   },
   "source": [
    "There are quite a few things, all of which are important for solving the robotics environments! For in-depth details on the exact data stored here, see the docs: https://haosulab.github.io/ManiSkill2/concepts/observation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJdClsNmz4o"
   },
   "source": [
    "### 2.1 Vectorized Environments on the GPU\n",
    "\n",
    "For learning and evaluation, vectorized environments enable you to take actions and receive observations in parallel and accelerating everything. As a result, vectorized environments are critical for fast reinforcement learning.\n",
    "\n",
    "We provide a custom `VecEnv` object and `make_vec_env` function to additionally optimize this vectorization onto the GPU. Note that this is different from the typical `SubProcEnv` used by libraries such as SB3 which do not have a GPU optimization.\n",
    "\n",
    "For vectorized environments that return visual observations, these observations are kept on the GPU as PyTorch cuda tensors as shown below and now have an extra batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTh2plX2mzOF",
    "outputId": "7f2c9174-2b7b-4719-f40e-24bc792bd8ae"
   },
   "source": [
    "from mani_skill2.vector import VecEnv, make as make_vec_env\n",
    "num_envs = 2 # recommended value for Google Colab. If you have more cores and a more powerful GPU you can increase this\n",
    "env: VecEnv = make_vec_env(\n",
    "    env_id,\n",
    "    num_envs,\n",
    "    obs_mode=obs_mode,\n",
    "    reward_mode=reward_mode,\n",
    "    control_mode=control_mode,\n",
    ")\n",
    "obs, _ = env.reset(seed=0)\n",
    "print(\"Base Camera RGB:\", obs['image']['base_camera']['rgb'].shape)\n",
    "print(\"Base Camera RGB device:\", obs['image']['base_camera']['rgb'].device)\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "While observations are kept as PyTorch cuda tensors, jax tensors are also supported and with minimal overhead you can change the cuda tensors to jax tensors using the code below\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.dlpack\n",
    "import torch\n",
    "import torch.utils.dlpack\n",
    "\n",
    "def jax_to_torch(x):\n",
    "    return torch.utils.dlpack.from_dlpack(jax.dlpack.to_dlpack(x))\n",
    "def torch_to_jax(x):\n",
    "    return jax.dlpack.from_dlpack(torch.utils.dlpack.to_dlpack(x))\n",
    "\n",
    "jax_obs = torch_to_jax(obs)\n",
    "```"
   ],
   "metadata": {
    "id": "mZovQ8j9Ey_V"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdnjOKZ5ml3I"
   },
   "source": [
    "### 2.2 Adapting ManiSkill environments for Stable Baselines with an Observation Wrapper\n",
    "\n",
    "If you look at the observation space and the returned observation, you will notice that it is a nested dictionary with different values in there, including both image data from cameras and state data about the robot. If you use the GPU optimized VecEnv then some of it is on the GPU (SB3 expects numpy observations)\n",
    "\n",
    "SB3 won't be able to use this out of the box so we can define a custom observation wrapper to make the ManiSkill environment conform with SB3. Here, we are simply going to take the two RGB images, two depth images from both cameras (base camera and hand camera) and the state data and create a workable observation for SB3.\n",
    "\n",
    "Feel free to the use the code as is and skip this section. If you want additional customization such as using robot segmentation data, point cloud data etc., you will need to edit the below wrapper appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G8dAIBkMPdoJ"
   },
   "source": [
    "from mani_skill2.vector.vec_env import VecEnvObservationWrapper\n",
    "from mani_skill2.utils.common import flatten_dict_space_keys, flatten_state_dict\n",
    "\n",
    "class ManiSkillRGBDWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert env.obs_mode == \"rgbd\"\n",
    "        self.observation_space = self.init_observation_space(env.observation_space)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_observation_space(obs_space: spaces.Dict):\n",
    "        # States include robot proprioception (agent) and task information (extra)\n",
    "        # NOTE: SB3 does not support nested observation spaces, so we convert them to flat spaces\n",
    "        state_spaces = []\n",
    "        state_spaces.extend(flatten_dict_space_keys(obs_space[\"agent\"]).spaces.values())\n",
    "        state_spaces.extend(flatten_dict_space_keys(obs_space[\"extra\"]).spaces.values())\n",
    "        # Concatenate all the state spaces\n",
    "        state_size = sum([space.shape[0] for space in state_spaces])\n",
    "        state_space = spaces.Box(-np.inf, np.inf, shape=(state_size,))\n",
    "\n",
    "        # Concatenate all the image spaces\n",
    "        image_shapes = []\n",
    "        for cam_uid in obs_space[\"image\"]:\n",
    "            cam_space = obs_space[\"image\"][cam_uid]\n",
    "            image_shapes.append(cam_space[\"rgb\"].shape)\n",
    "            image_shapes.append(cam_space[\"depth\"].shape)\n",
    "        image_shapes = np.array(image_shapes)\n",
    "        assert np.all(image_shapes[0, :2] == image_shapes[:, :2]), image_shapes\n",
    "        h, w = image_shapes[0, :2]\n",
    "        c = image_shapes[:, 2].sum(0)\n",
    "        rgbd_space = spaces.Box(0, np.inf, shape=(h, w, c))\n",
    "\n",
    "        # Create the new observation space\n",
    "        return spaces.Dict({\"rgbd\": rgbd_space, \"state\": state_space})\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_observation(observation):\n",
    "        # Process images. RGB is normalized to [0, 1].\n",
    "        images = []\n",
    "        for cam_uid, cam_obs in observation[\"image\"].items():\n",
    "            rgb = cam_obs[\"rgb\"] / 255.0\n",
    "            depth = cam_obs[\"depth\"]\n",
    "\n",
    "            # NOTE: SB3 does not support GPU tensors, so we transfer them to CPU.\n",
    "            # For other RL frameworks that natively support GPU tensors, this step is not necessary.\n",
    "            if isinstance(rgb, th.Tensor):\n",
    "                rgb = rgb.to(device=\"cpu\", non_blocking=True)\n",
    "            if isinstance(depth, th.Tensor):\n",
    "                depth = depth.to(device=\"cpu\", non_blocking=True)\n",
    "\n",
    "            images.append(rgb)\n",
    "            images.append(depth)\n",
    "\n",
    "        # Concatenate all the images\n",
    "        rgbd = np.concatenate(images, axis=-1)\n",
    "\n",
    "        # Concatenate all the states\n",
    "        state = np.hstack(\n",
    "            [\n",
    "                flatten_state_dict(observation[\"agent\"]),\n",
    "                flatten_state_dict(observation[\"extra\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return dict(rgbd=rgbd, state=state)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return self.convert_observation(observation)\n",
    "\n",
    "# We separately define an VecEnv observation wrapper for the ManiSkill VecEnv\n",
    "# as the gpu optimization makes it incompatible with the SB3 wrapper\n",
    "class ManiSkillRGBDVecEnvWrapper(VecEnvObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "\n",
    "        assert env.obs_mode == \"rgbd\"\n",
    "        # we simply define the single env observation space. The inherited wrapper automatically computes the batched version\n",
    "        single_observation_space = ManiSkillRGBDWrapper.init_observation_space(\n",
    "            env.single_observation_space\n",
    "        )\n",
    "        super().__init__(env, single_observation_space)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return ManiSkillRGBDWrapper.convert_observation(observation)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r7hmN2inRSM"
   },
   "source": [
    "We can now wrap the original environment and we'll see the returned observations are now more compact and importantly usable by SB3. Note that the rgbd information is now a numpy array as opposed to a PyTorch tensor on the GPU as we converted with the wrapper for SB3. For faster RL, you can use an RL library that supports observations on the GPU like [RL-Games](https://github.com/Denys88/rl_games) or our own library [ManiSkill2-Learn](https://github.com/haosulab/ManiSkill2-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlNEXHV9S2db",
    "outputId": "9a5c7e78-b4be-4a4b-9a5a-a169dfee2aa3"
   },
   "source": [
    "num_envs = 2\n",
    "env: VecEnv = make_vec_env(\n",
    "    env_id,\n",
    "    num_envs,\n",
    "    obs_mode=obs_mode,\n",
    "    control_mode=control_mode\n",
    ")\n",
    "# use the VecEnvWrapper we created earlier for RGBD data\n",
    "env = ManiSkillRGBDVecEnvWrapper(env)\n",
    "obs, _ = env.reset(seed=0)\n",
    "print(obs.keys())\n",
    "print(\"rgbd shape\", obs[\"rgbd\"].shape)\n",
    "print(\"rgbd type\", type(obs[\"rgbd\"]))\n",
    "print(\"state shape\", obs[\"state\"].shape)\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNF39XEAnVlm"
   },
   "source": [
    "### 2.3 Creating a model to process RGBD and State data\n",
    "\n",
    "SB3 natively doesn't support processing RGB data with depth information, so we will need to create a custom network to process that data. We can make use of the SB3 BaseExtractor class to do this so we can fit our model into any of SB3's algorithms. For more details on feature extractors see the SB3 docs: https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#custom-feature-extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SsgrC7ocnUye"
   },
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "class CustomExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict):\n",
    "        super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "        extractors = {}\n",
    "\n",
    "        total_concat_size = 0\n",
    "        feature_size = 256\n",
    "\n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            # We go through all subspaces in the observation space.\n",
    "            # We know there will only be \"rgbd\" and \"state\", so we handle those below\n",
    "            if key == \"rgbd\":\n",
    "                # here we use a NatureCNN architecture to process images, but any architecture is permissble here\n",
    "                in_channels = subspace.shape[-1]\n",
    "                cnn = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4, padding=0),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Flatten()\n",
    "                )\n",
    "\n",
    "                # to easily figure out the dimensions after flattening, we pass a test tensor\n",
    "                test_tensor = th.zeros([subspace.shape[2], subspace.shape[0], subspace.shape[1]])\n",
    "                with th.no_grad():\n",
    "                    n_flatten = cnn(test_tensor[None]).shape[1]\n",
    "                fc = nn.Sequential(nn.Linear(n_flatten, feature_size), nn.ReLU())\n",
    "                extractors[\"rgbd\"] = nn.Sequential(cnn, fc)\n",
    "                total_concat_size += feature_size\n",
    "            elif key == \"state\":\n",
    "                # for state data we simply pass it through a single linear layer\n",
    "                state_size = subspace.shape[0]\n",
    "                extractors[\"state\"] = nn.Linear(state_size, 64)\n",
    "                total_concat_size += 64\n",
    "\n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "        self._features_dim = total_concat_size\n",
    "\n",
    "    def forward(self, observations) -> th.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "        # self.extractors contain nn.Modules that do all the processing.\n",
    "        for key, extractor in self.extractors.items():\n",
    "            if key == \"rgbd\":\n",
    "                observations[key] = observations[key].permute((0, 3, 1, 2))\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "        return th.cat(encoded_tensor_list, dim=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iubRq3ui8Vr1"
   },
   "source": [
    "We have a custom model / feature extractor ready, an observation wrapper that allows SB3 to work with ManiSkill environments, all that is left now is to setup an RL agent and train, evaluate, and monitor it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioewgi6rPhNa"
   },
   "source": [
    "### 2.4 Setting up Training and Evaluation environments\n",
    "\n",
    "First, we will create vectorized environments to speed up training by using the `make_vec_env` function from earlier. We will also make evaluation environments to record evaluation videos during and after training.\n",
    "\n",
    "Additionally, note that we also use the `SB3VecEnvWrapper` wrapper which we provide. This is to format the Maniskill `VecEnv` into a SB3 compatible one so SB3 wrappers like `VecMonitor` can also work, too.\n",
    "\n",
    "Note that currently Maniskill `VecEnv` does not support rendering with `env.render`. So we will need to create a `make_env` function that uses `gym.make` to create environments as well for evaluation purposes and recording videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2glrHcXoX3c",
    "outputId": "9ad38a8e-d352-4aca-a5b8-ad1787414482"
   },
   "source": [
    "from mani_skill2.vector.wrappers.sb3 import SB3VecEnvWrapper\n",
    "from mani_skill2.utils.wrappers import RecordEpisode\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from functools import partial\n",
    "\n",
    "num_envs = 2 # you can increases this if you have more cores to speed up training\n",
    "env_id = \"LiftCube-v0\"\n",
    "obs_mode = \"rgbd\"\n",
    "control_mode = \"pd_ee_delta_pose\"\n",
    "reward_mode = \"normalized_dense\" # this the default reward mode which is a dense reward scaled to [0, 1]\n",
    "\n",
    "# define a make_env function for Stable Baselines\n",
    "def make_env(env_id: str, max_episode_steps=None, record_dir: str = None):\n",
    "    # NOTE: Import envs here so that they are registered with gym in subprocesses\n",
    "    import mani_skill2.envs\n",
    "\n",
    "    env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode, reward_mode=reward_mode, max_episode_steps=max_episode_steps, render_mode=\"cameras\")\n",
    "    # For training, we regard the task as a continuous task with infinite horizon.\n",
    "    # you can use the ContinuousTaskWrapper here for that\n",
    "    if max_episode_steps is not None:\n",
    "        env = ContinuousTaskWrapper(env)\n",
    "    env = ManiSkillRGBDWrapper(env)\n",
    "    # For evaluation, we record videos\n",
    "    if record_dir is not None:\n",
    "        env = SuccessInfoWrapper(env)\n",
    "        env = RecordEpisode(env, record_dir, save_trajectory=False, info_on_video=True)\n",
    "    return env\n",
    "\n",
    "# create one eval environment. Partial must be used here\n",
    "env_fn = partial(\n",
    "    make_env,\n",
    "    env_id,\n",
    "    record_dir=\"logs/videos\",\n",
    ")\n",
    "eval_env = SubprocVecEnv([env_fn for i in range(1)])\n",
    "eval_env = VecMonitor(eval_env) # attach this so SB3 can log reward metrics\n",
    "eval_env.seed(0)\n",
    "eval_env.reset()\n",
    "\n",
    "# create num_envs training environments, with max_episode_steps=50\n",
    "# instead of the default 200 to speed up training\n",
    "env: VecEnv = make_vec_env(\n",
    "    env_id,\n",
    "    num_envs,\n",
    "    obs_mode=obs_mode,\n",
    "    reward_mode=reward_mode,\n",
    "    control_mode=control_mode,\n",
    "    max_episode_steps=50,\n",
    "    # specify wrappers for each individual environment e.g here we specify the\n",
    "    # Continuous task wrapper and pass in the max_episode_steps parameter via the partial tool\n",
    "    wrappers=[\n",
    "        ContinuousTaskWrapper\n",
    "    ]\n",
    ")\n",
    "env = ManiSkillRGBDVecEnvWrapper(env)\n",
    "# use the maniskill provided SB3VecEnvWrapper to make the environment compatible with SB3\n",
    "env = SB3VecEnvWrapper(env)\n",
    "env = VecMonitor(env)\n",
    "env.seed(0)\n",
    "obs = env.reset()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGO65RDTo9ba"
   },
   "source": [
    "To help monitor our training, we can create an evaluation callback function as well as a checkpoint callback function using SB3. The evaluation callback will periodically evaluate the agent without noise and save results to the logs folder. The `eval_env` will also save videos to `logs/videos`. Whenever the evaluation reward has improved, it will save a new best model as well. Finally, the checkpoint callback will periodically save the training progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "U4boL-jVo6up"
   },
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
    "                         log_path=\"./logs/\", eval_freq=32000,\n",
    "                         deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=32000,\n",
    "    save_path=\"./logs/\",\n",
    "    name_prefix=\"rl_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKDwLPiAvEoE"
   },
   "source": [
    "### 2.5 RL Training with PPO\n",
    "Finally, we can begin training. We have to first define the policy and training configuration. The configs provided are already tuned and will be able to train out a succesful LiftCube policy. The configs are set to perform a rollout of 3200 steps split across each parallel environment, and update the policy with batch size 400 for 5 epochs. All logs, including tensorboard logs (run `tensorboard --logdir logs` to vie wthem) and evaluation videos are stored in the `logs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcPfMGLj1U4L",
    "outputId": "be3d7c96-4ac3-4714-ff0e-ad6c5e3de66f"
   },
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "set_random_seed(0) # set SB3's global seed to 0\n",
    "rollout_steps = 3200\n",
    "\n",
    "# create our model\n",
    "policy_kwargs = dict(features_extractor_class=CustomExtractor, net_arch=[256, 128])\n",
    "model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,\n",
    "    n_steps=rollout_steps // num_envs, batch_size=400,\n",
    "    n_epochs=5,\n",
    "    tensorboard_log=\"./logs\",\n",
    "    gamma=0.8,\n",
    "    target_kl=0.2\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VNtXkq51tvS"
   },
   "source": [
    "Run the next cell to train the model for 160,000+ steps. The final model is then saved to `logs/latest_model`. This can take 15 min to 1 hour (depending on your machine) to finish training. To speed up training you can use a computer with more CPU cores and/or a more powerful GPU.\n",
    "\n",
    "To keep track of training progress you can go to `logs/videos` and download the evaluation videos saved during training.\n",
    "\n",
    "We have also provided weights trained through this colab tutorial so if you wish to skip the training time  you can skip the next cell and run the following one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sai8lHHcvG5J"
   },
   "source": [
    "# Train with PPO\n",
    "model.learn(128_000, callback=[checkpoint_callback, eval_callback])\n",
    "model.save(\"./logs/latest_model\")\n",
    "\n",
    "# optionally load back the model that was saved\n",
    "model = model.load(\"./logs/latest_model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "dN0BinYA1Kcn"
   },
   "source": [
    "# Code for simply loading a pretrained policy\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://huggingface.co/datasets/haosulab/ManiSkill2/resolve/main/pretrained_models/tutorials/LiftCube-v0_rl.rgbd.pd_ee_delta_pose.zip\",\n",
    "    \"pretrained_model.zip\")\n",
    "model.policy = model.load(\"pretrained_model\").policy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4DkwBoZRSpu"
   },
   "source": [
    "### 2.6 Evaluation\n",
    "Once a model is trained, whether you ran the script above or downloaded the pretrained policy, you can run below to evaluate it and save some videos. You can set `render=True` if you have a GUI to create to live render the evaluation.\n",
    "\n",
    "If you use default configurations / use the pretrained model, you should get around 60%+ success rate on LiftCube. If you train longer you can get 90%+ success rate.\n",
    "\n",
    "**Note that the green sphere in the videos represents the target height to lift the cube to (not a goal position).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTdNiAmSZ6xF",
    "outputId": "19b7ad90-c877-439d-892c-1f9991364bf9"
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "eval_env.close() # close the old eval env\n",
    "# make a new one that saves to a different directory\n",
    "env_fn = partial(\n",
    "    make_env,\n",
    "    env_id,\n",
    "    record_dir=\"logs/eval_videos\",\n",
    ")\n",
    "eval_env = SubprocVecEnv([env_fn for i in range(1)])\n",
    "eval_env = VecMonitor(eval_env) # attach this so SB3 can log reward metrics\n",
    "eval_env.seed(0)\n",
    "eval_env.reset()\n",
    "\n",
    "returns, ep_lens = evaluate_policy(model, eval_env, deterministic=True, render=False, return_episode_rewards=True, n_eval_episodes=5)\n",
    "success = np.array(ep_lens) < 200 # episode length < 200 means we solved the task before time ran out\n",
    "success_rate = success.mean()\n",
    "print(f\"Success Rate: {success_rate}\")\n",
    "print(f\"Episode Lengths: {ep_lens}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "K9l6AjvCdGjj",
    "outputId": "1c1c993a-ca4c-4082-9521-b90eaf06f6a1"
   },
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./logs/eval_videos/0.mp4\", embed=True) # Watch one of the replays"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X-prXDwyLEF"
   },
   "source": [
    "## 3 Final Thoughts\n",
    "\n",
    "This tutorial demonstrates a simple approach to solving ManiSkill2 environments with state-based and visual-based Reinforcement Learning (RL). While LiftCube may appear solved already, other environments are much more complex and solving them scalably is an open problem!\n",
    "\n",
    "Imitation Learning (IL) is another technique which leverages the demonstration dataset of ManiSkill2 to help learn complex skills. For a tutorial on Imitation Learning with ManiSkill2, see our [IL Colab Tutorial](https://colab.research.google.com/github/haosulab/ManiSkill2/blob/tutorials/examples/tutorials/3_imitation_learning.ipynb)\n",
    "\n",
    "While our environments and code enable much faster visual-based RL and IL compared to other robotics environments, there are still a number of approaches that can enhance visual-based policies. Many of these approaches have been consolidated into our own library called [ManiSkill2-Learn](https://github.com/haosulab/ManiSkill2-Learn) which has code to leverage RGBD and PointClouds, transformers, and more.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1T7S6wmryEub"
   ],
   "provenance": [],
   "include_colab_link": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "mani_skill2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "27bbf14ba245321bda1133d220a06c17414b7fdf9e1ceccf1e40995cf2d3671b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
